{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddf33dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ad445",
   "metadata": {},
   "source": [
    "<h1>Load dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc1e85d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine \n",
      "['\\t', '\\n', ' ', '!', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'À', 'Æ', 'Ç', 'É', 'à', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'œ', '—', '‘', '’', '“', '”', '…', '\\ufeff']\n",
      "vocab_size: 101\n"
     ]
    }
   ],
   "source": [
    "dataset_file_path = \"data/The Complete Works of William Shakespeare.txt\"\n",
    "\n",
    "with open(dataset_file_path, 'r', encoding= 'utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[1500:1600])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "# encoder and decoder\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "\n",
    "# convert dataset from text to index of char\n",
    "dataset = np.array([char_to_idx[ch] for ch in text], dtype= np.long)\n",
    "\n",
    "# save a bit of memory why not\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4a1c9",
   "metadata": {},
   "source": [
    "## Crate Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2990eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "CONTEXT_LENGHT = 64\n",
    "EMBEDDING_DIM = 32\n",
    "ATTENTION_HEAD_DIM = 16\n",
    "MLP_HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb16075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This LLM transformer do SA --> MLP --> SA --> MLP\n",
    "\n",
    "def init_parameters(vocab_size: int,\n",
    "                    embedding_dim: int,\n",
    "                    attention_head_dim: int,\n",
    "                    mlp_hidden_dim: int,\n",
    "                    scaling_factor: float) -> dict[str:np.array]:\n",
    "    \"\"\"\n",
    "    Initialize all model parameters (Only for this LLM or)\n",
    "\n",
    "    Parameters\n",
    "    vocab_size:         int\n",
    "    embeding_dim:       int\n",
    "    attention_head_dim: int\n",
    "    mlp_hidden_dim:     int\n",
    "    scaling_factor:     int\n",
    "\n",
    "    Output\n",
    "    parameters: dict[str:np.array]\n",
    "    parameters that contain the weight and bias matrix (np.array) of all\n",
    "    LLM parameters, can be access by name (str) of parameter\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "\n",
    "    # Map token index to embedding vector\n",
    "    parameters['embedding'] = np.random.randn(vocab_size, embedding_dim) * scaling_factor           # (vocab_size, embed)\n",
    "\n",
    "    # 1st SA\n",
    "    parameters['W_k1'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_q1'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_Vup1'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor      # (embed, attend)\n",
    "    parameters['W_Vdown1'] = np.random.randn(attention_head_dim, embedding_dim) * scaling_factor    # (attend, embed)\n",
    "\n",
    "    # 1st MLP\n",
    "    parameters['W_mlp1_up'] = np.random.randn(embedding_dim, mlp_hidden_dim) * scaling_factor       # (embed, hidden)\n",
    "    parameters['b_mlp1_up'] = np.zeros((1, mlp_hidden_dim))                                         # (1, hidden)\n",
    "    parameters['W_mlp1_down'] = np.random.randn(mlp_hidden_dim, embedding_dim) * scaling_factor     # (hidden, embed)\n",
    "    parameters['b_mlp1_down'] = np.zeros((1, embedding_dim))                                        # (1, embed)\n",
    "\n",
    "    # 2nd SA\n",
    "    parameters['W_k2'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_q2'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_Vup2'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor      # (embed, attend)\n",
    "    parameters['W_Vdown2'] = np.random.randn(attention_head_dim, embedding_dim) * scaling_factor    # (attend, embed)\n",
    "\n",
    "    # 2nd MLP\n",
    "    parameters['W_mlp2_up'] = np.random.randn(embedding_dim, mlp_hidden_dim) * scaling_factor       # (embed, hidden)\n",
    "    parameters['b_mlp2_up'] = np.zeros((1, mlp_hidden_dim))                                         # (1, hidden)\n",
    "    parameters['W_mlp2_down'] = np.random.randn(mlp_hidden_dim, embedding_dim) * scaling_factor     # (hidden, embed)\n",
    "    parameters['b_mlp2_down'] = np.zeros((1, embedding_dim))                                        # (1, embed)\n",
    "\n",
    "    # Unembedding\n",
    "    parameters['unembedding'] = np.random.randn(embedding_dim, vocab_size) * scaling_factor         # (embed, vocab_size)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d73e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = init_parameters(vocab_size, EMBEDDING_DIM, ATTENTION_HEAD_DIM, MLP_HIDDEN_DIM, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eac8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: (101, 32)\n",
      "W_k1: (32, 16)\n",
      "W_q1: (32, 16)\n",
      "W_Vup1: (32, 16)\n",
      "W_Vdown1: (16, 32)\n",
      "W_mlp1_up: (32, 128)\n",
      "b_mlp1_up: (1, 128)\n",
      "W_mlp1_down: (128, 32)\n",
      "b_mlp1_down: (1, 32)\n",
      "W_k2: (32, 16)\n",
      "W_q2: (32, 16)\n",
      "W_Vup2: (32, 16)\n",
      "W_Vdown2: (16, 32)\n",
      "W_mlp2_up: (32, 128)\n",
      "b_mlp2_up: (1, 128)\n",
      "W_mlp2_down: (128, 32)\n",
      "b_mlp2_down: (1, 32)\n",
      "unembedding: (32, 101)\n",
      "Total number of parameters: 27264\n"
     ]
    }
   ],
   "source": [
    "sum_of_parameters = 0\n",
    "for key, value in parameters.items():\n",
    "    print(f'{key}: {value.shape}')\n",
    "    sum_of_parameters += value.shape[0] * value.shape[1]\n",
    "print(f'Total number of parameters: {sum_of_parameters}')\n",
    "del sum_of_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0284ed",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782158",
   "metadata": {},
   "source": [
    "$\\text{let}\\ v = \\begin{bmatrix}x_1\\ x_2 \\ ...\\ x_n\\end{bmatrix}\\\\\\text{softmax}(v) = \\begin{bmatrix}\\frac{e^{x_1}}{\\Sigma_{i=1}^ne^{x_i}}\\ \\frac{e^{x_2}}{\\Sigma_{i=1}^ne^{x_i}}\\ ...\\ \\frac{e^{x_n}}{\\Sigma_{i=1}^ne^{x_i}}\\end{bmatrix}$\n",
    "\n",
    "$\\text{ReLU}(x) = \\begin{cases}x, &x>0\\\\0,&x\\leq0\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1387518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(matrix: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute softmax value for each row\n",
    "\n",
    "    Input\n",
    "    matrix: np.array\n",
    "\n",
    "    Output\n",
    "    matrix: np.array\n",
    "    \"\"\"\n",
    "    # Prevent overflow but still have the same softmax value\n",
    "    exp_matrix = np.exp(matrix - np.max(matrix, axis=-1, keepdims=True))\n",
    "    return exp_matrix / np.sum(exp_matrix, axis=-1, keepdims=True)\n",
    "\n",
    "def relu(matrix: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute ReLU for each value in the matrix\n",
    "\n",
    "    Input\n",
    "    matrix: np.array\n",
    "\n",
    "    Output\n",
    "    matrix: np.array\n",
    "    \"\"\"\n",
    "    return np.maximum(0, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d1096",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b77b6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_casual_mask(seq_len: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Crate casual mask for self-attention\n",
    "\n",
    "    Input\n",
    "    seq_len:    int\n",
    "\n",
    "    Output\n",
    "    masking: np.array[[bool]]\n",
    "    \"\"\"\n",
    "    mask = np.triu(np.ones(seq_len, seq_len))\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23c41a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_all_position(probability:    np.array,\n",
    "                                    target_tokens:  np.array) -> float:\n",
    "    \"\"\"\n",
    "    Map the result with prediction and return the average cross entropy loss of all position\n",
    "\n",
    "    Input\n",
    "    probability:    np.array\n",
    "    target_tokens:  np.array\n",
    "\n",
    "    Output\n",
    "    average_error:  float\n",
    "    \"\"\"\n",
    "    correct_probability = probability[np.arange(len(target_tokens)), target_tokens]\n",
    "\n",
    "    losses = -np.log(correct_probability + 1e-8) # Add 1e-8 for numerical stability\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944e450",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98792860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_block(context:   np.array,\n",
    "                         W_k:       np.array,\n",
    "                         W_q:       np.array,\n",
    "                         W_Vup:     np.array,\n",
    "                         W_Vdown:   np.array) -> tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Self-attention block\n",
    "\n",
    "    Input\n",
    "    context:   np.array,\n",
    "    W_k:       np.array,\n",
    "    W_q:       np.array,\n",
    "    W_Vup:     np.array,\n",
    "    W_Vdown:   np.array,\n",
    "\n",
    "    Output\n",
    "    tuple[output, attention_weight]\n",
    "    \"\"\"\n",
    "    context_lenght, embedding_dim = context.shape # (context_lenght, embedding_dim)\n",
    "\n",
    "    K = context @ W_k # K: (context_lenght, attention_head_dim)\n",
    "    Q = context @ W_q # Q: (context_lenght, attention_head_dim)\n",
    "\n",
    "    attention_scores = (Q @ K.T) / np.sqrt(Q.shape[0]) # (context_lenght, context_lenght)\n",
    "\n",
    "    # Masking attention scores by replace the lower triangle with -inf\n",
    "    causal_mask = crate_casual_mask(context_lenght)\n",
    "    attention_scores = np.where(causal_mask, attention_scores, -1e9)\n",
    "\n",
    "    attention_weights = softmax(attention_scores)\n",
    "\n",
    "    V_up = context @ W_Vup # (context_lenght, attention_head_dim)\n",
    "\n",
    "    attention_output = attention_weights @ V_up # (context_lenght, attention_head_dim)\n",
    "\n",
    "    output = attention_output @ W_Vdown # (context_lenght, embedding_dim)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1bb3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_block(context:  np.array,\n",
    "              W_up:     np.array,\n",
    "              b_up:     np.array,\n",
    "              W_down:   np.array,\n",
    "              b_down:   np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    MLP block\n",
    "\n",
    "    Input\n",
    "    context: np.array\n",
    "    W_up:    np.array\n",
    "    b_up:    np.array\n",
    "    W_down:  np.array\n",
    "    b_down:  np.array\n",
    "\n",
    "    Output\n",
    "    output: np.array\n",
    "    \"\"\"\n",
    "    hidden = context @ W_up + b_up\n",
    "    hidden = relu(hidden)\n",
    "    output = hidden @ W_down + b_down\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99007b",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f365ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(tokens: np.array,\n",
    "                 parameters: dict[str: np.array]) -> tuple[np.array, dict[str: np.array]]:\n",
    "    \"\"\"\n",
    "    Complete forward pass\n",
    "\n",
    "    Input\n",
    "    tokens:     np.array\n",
    "    parameters: dict[str: np.array]\n",
    "\n",
    "    Output\n",
    "    probability of each token:  np.array\n",
    "    cache: dict[str: np.array]\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Embedding lookup\n",
    "    token = parameters['embedding'][tokens]\n",
    "\n",
    "    cache = {'token_0': token.copy()}\n",
    "\n",
    "    # 2. 1st SA block\n",
    "    token_sa1, attention_weights1 = self_attention_block(token,\n",
    "                                                         parameters['W_k1'],\n",
    "                                                         parameters['W_q1'],\n",
    "                                                         parameters['W_Vup1'],\n",
    "                                                         parameters['W_Vdown1'])\n",
    "    cache['token_sa1'] = token_sa1.copy()\n",
    "    cache['attention_weights1'] = attention_weights1.copy()\n",
    "\n",
    "    token = token + token_sa1\n",
    "\n",
    "    # 3. 1st MLP block\n",
    "    token_mlp1 = mlp_block(token,\n",
    "                           parameters['W_mlp1_up'],\n",
    "                           parameters['b_mlp1_up'],\n",
    "                           parameters['W_mlp1_down'],\n",
    "                           parameters['b_mlp1_down'])\n",
    "    cache['token_mlp1'] = token_mlp1.copy()\n",
    "\n",
    "    token = token + token_mlp1\n",
    "\n",
    "    # 4. 2nd SA block\n",
    "    token_sa2, attention_weights2 = self_attention_block(token,\n",
    "                                                         parameters['W_k2'],\n",
    "                                                         parameters['W_q2'],\n",
    "                                                         parameters['W_Vup2'],\n",
    "                                                         parameters['W_Vdown2'])\n",
    "    cache['token_sa2'] = token_sa2.copy()\n",
    "    cache['attention_weights2'] = attention_weights2.copy()\n",
    "\n",
    "    token = token + token_sa2\n",
    "\n",
    "    # 5. 2nd MLP block\n",
    "    token_mlp2 = mlp_block(token,\n",
    "                           parameters['W_mlp2_up'],\n",
    "                           parameters['b_mlp2_up'],\n",
    "                           parameters['W_mlp2_down'],\n",
    "                           parameters['b_mlp2_down'])\n",
    "    cache['token_mlp2'] = token_mlp2.copy()\n",
    "\n",
    "    token = token + token_mlp2\n",
    "\n",
    "    # 6. Prediction (unembedding)\n",
    "    logits = token @ parameters['unembedding'] # (32, vocab_size)\n",
    "    probability = softmax(logits)\n",
    "\n",
    "    cache['logits'] = logits.copy()\n",
    "    cache['probability'] = probability.copy()\n",
    "    cache['final_embedding'] = token.copy()\n",
    "\n",
    "    return probability, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d03a87",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6613b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data:             np.array,\n",
    "              batch_size:       int,\n",
    "              context_length:   int) -> tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Get a batch of training examples\n",
    "\n",
    "    Input\n",
    "    data:           np.array\n",
    "    batch_size:     int\n",
    "    context_length: int\n",
    "\n",
    "    Output\n",
    "    (contexts, targets)\n",
    "    contexts:   np.array\n",
    "    targets:    np.array\n",
    "    \"\"\"\n",
    "\n",
    "    indices = np.random.randint(0, len(data) - context_length - 1, batch_size)\n",
    "\n",
    "    contexts = np.zeros((batch_size, context_length), dtype=np.int32)\n",
    "    targets = np.zeros(batch_size, dtype=np.int32)\n",
    "\n",
    "    for index, data_index in enumerate(indices):\n",
    "        contexts[index] = data[data_index: data_index + context_length]\n",
    "        targets[index] = data[data_index + 1: data_index + context_length + 1]\n",
    "    \n",
    "    return contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c0be82f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 26) (894065645.py, line 26)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mIT'S NOT DONE YET\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 26)\n"
     ]
    }
   ],
   "source": [
    "def train_step_batch(context_tokens:    np.array,\n",
    "                     target_tokens:     np.array,\n",
    "                     parameters:        dict[str: np.array],\n",
    "                     learning_rate:     float   = 0.001) -> tuple[float, np.array]:\n",
    "    \"\"\"\n",
    "    Perform one training step for all positions\n",
    "    \n",
    "\n",
    "    Input\n",
    "    context_tokens: np.array\n",
    "    target_tokens:  np.array\n",
    "    parameters:     dict[str : np.array]\n",
    "    learning_rateL  float\n",
    "\n",
    "    Output\n",
    "    (loss, probability)\n",
    "    loss:           float\n",
    "    probability:    np.array\n",
    "    \"\"\"\n",
    "    # Forward pass yay\n",
    "    probability, cache = forward_pass(context_tokens, parameters)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = cross_entropy_loss_all_position(probability, target_tokens)\n",
    "\n",
    "    IT'S NOT DONE YET"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
