{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddf33dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ad445",
   "metadata": {},
   "source": [
    "<h1>Load dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc1e85d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine \n",
      "['\\t', '\\n', ' ', '!', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'À', 'Æ', 'Ç', 'É', 'à', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'œ', '—', '‘', '’', '“', '”', '…', '\\ufeff']\n",
      "vocab_size: 101\n"
     ]
    }
   ],
   "source": [
    "dataset_file_path = \"data/The Complete Works of William Shakespeare.txt\"\n",
    "\n",
    "with open(dataset_file_path, 'r', encoding= 'utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[1500:1600])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "# encoder and decoder\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "\n",
    "# convert dataset from text to index of char\n",
    "dataset = np.array([char_to_idx[ch] for ch in text], dtype= np.long)\n",
    "\n",
    "# save a bit of memory why not\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4a1c9",
   "metadata": {},
   "source": [
    "## Crate Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2990eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "CONTEXT_LENGHT = 32\n",
    "EMBEDDING_DIM = 16\n",
    "ATTENTION_HEAD_DIM = 8\n",
    "MLP_HIDDEN_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb16075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This LLM transformer do SA --> MLP --> SA --> MLP\n",
    "\n",
    "def init_parameters(vocab_size: int,\n",
    "                    embedding_dim: int,\n",
    "                    attention_head_dim: int,\n",
    "                    mlp_hidden_dim: int,\n",
    "                    scaling_factor: float) -> dict[str:np.array]:\n",
    "    \"\"\"\n",
    "    Initialize all model parameters (Only for this LLM or)\n",
    "\n",
    "    Parameters\n",
    "    vocab_size:         int\n",
    "    embeding_dim:       int\n",
    "    attention_head_dim: int\n",
    "    mlp_hidden_dim:     int\n",
    "    scaling_factor:     int\n",
    "\n",
    "    Output\n",
    "    parameters: dict[str:np.array]\n",
    "    parameters that contain the weight and bias matrix (np.array) of all\n",
    "    LLM parameters, can be access by name (str) of parameter\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "\n",
    "    # Map token index to embedding vector\n",
    "    parameters['embedding'] = np.random.randn(vocab_size, embedding_dim) * scaling_factor           # (vocab_size, embed)\n",
    "\n",
    "    # 1st SA\n",
    "    parameters['W_k1'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_q1'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_Vup1'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor      # (embed, attend)\n",
    "    parameters['W_Vdown1'] = np.random.randn(attention_head_dim, embedding_dim) * scaling_factor    # (attend, embed)\n",
    "\n",
    "    # 1st MLP\n",
    "    parameters['W_mlp1_up'] = np.random.randn(embedding_dim, mlp_hidden_dim) * scaling_factor       # (embed, hidden)\n",
    "    parameters['b_mlp1_up'] = np.zeros((1, mlp_hidden_dim))                                         # (1, hidden)\n",
    "    parameters['W_mlp1_down'] = np.random.randn(mlp_hidden_dim, embedding_dim) * scaling_factor     # (hidden, embed)\n",
    "    parameters['b_mlp1_down'] = np.zeros((1, embedding_dim))                                        # (1, embed)\n",
    "\n",
    "    # 2nd SA\n",
    "    parameters['W_k2'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_q2'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor        # (embed, attend)\n",
    "    parameters['W_Vup2'] = np.random.randn(embedding_dim, attention_head_dim) * scaling_factor      # (embed, attend)\n",
    "    parameters['W_Vdown2'] = np.random.randn(attention_head_dim, embedding_dim) * scaling_factor    # (attend, embed)\n",
    "\n",
    "    # 2nd MLP\n",
    "    parameters['W_mlp2_up'] = np.random.randn(embedding_dim, mlp_hidden_dim) * scaling_factor       # (embed, hidden)\n",
    "    parameters['b_mlp2_up'] = np.zeros((1, mlp_hidden_dim))                                         # (1, hidden)\n",
    "    parameters['W_mlp2_down'] = np.random.randn(mlp_hidden_dim, embedding_dim) * scaling_factor     # (hidden, embed)\n",
    "    parameters['b_mlp2_down'] = np.zeros((1, embedding_dim))                                        # (1, embed)\n",
    "\n",
    "    # Unembedding\n",
    "    parameters['unembedding'] = np.random.randn(embedding_dim, vocab_size) * scaling_factor         # (embed, vocab_size)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d73e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = init_parameters(vocab_size, EMBEDDING_DIM, ATTENTION_HEAD_DIM, MLP_HIDDEN_DIM, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5eac8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding: (101, 16)\n",
      "W_k1: (16, 8)\n",
      "W_q1: (16, 8)\n",
      "W_Vup1: (16, 8)\n",
      "W_Vdown1: (8, 16)\n",
      "W_mlp1_up: (16, 64)\n",
      "b_mlp1_up: (1, 64)\n",
      "W_mlp1_down: (64, 16)\n",
      "b_mlp1_down: (1, 16)\n",
      "W_k2: (16, 8)\n",
      "W_q2: (16, 8)\n",
      "W_Vup2: (16, 8)\n",
      "W_Vdown2: (8, 16)\n",
      "W_mlp2_up: (16, 64)\n",
      "b_mlp2_up: (1, 64)\n",
      "W_mlp2_down: (64, 16)\n",
      "b_mlp2_down: (1, 16)\n",
      "unembedding: (16, 101)\n",
      "Total number of parameters: 8512\n"
     ]
    }
   ],
   "source": [
    "sum_of_parameters = 0\n",
    "for key, value in parameters.items():\n",
    "    print(f'{key}: {value.shape}')\n",
    "    sum_of_parameters += value.shape[0] * value.shape[1]\n",
    "print(f'Total number of parameters: {sum_of_parameters}')\n",
    "del sum_of_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0284ed",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782158",
   "metadata": {},
   "source": [
    "$\\text{let}\\ v = \\begin{bmatrix}x_1\\\\x_2\\\\...\\\\x_n\\end{bmatrix}\\\\\\text{softmax}(v) = \\begin{bmatrix}\\frac{e^{x_1}}{\\Sigma_{i=1}^ne^{x_i}}\\\\\\frac{e^{x_2}}{\\Sigma_{i=1}^ne^{x_i}}\\\\...\\\\\\frac{e^{x_n}}{\\Sigma_{i=1}^ne^{x_i}}\\end{bmatrix}$\n",
    "\n",
    "$\\text{ReLU}(x) = \\begin{cases}x, &x>0\\\\0,&x\\leq0\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1387518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(matrix: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute softmax value for each row\n",
    "\n",
    "    Input\n",
    "    matrix: np.array\n",
    "\n",
    "    Output\n",
    "    matrix: np.array\n",
    "    \"\"\"\n",
    "    # Prevent overflow but still have the same softmax value\n",
    "    exp_matrix = np.exp(matrix - np.max(matrix, axis=-1, keepdims=True))\n",
    "    return exp_matrix / np.sum(exp_matrix, axis=-1, keepdims=True)\n",
    "\n",
    "def relu(matrix: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute ReLU for each value in the matrix\n",
    "\n",
    "    Input\n",
    "    matrix: np.array\n",
    "\n",
    "    Output\n",
    "    matrix: np.array\n",
    "    \"\"\"\n",
    "    return np.maximum(0, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d1096",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b77b6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crate_casual_mask(seq_len: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Crate casual mask for self-attention\n",
    "\n",
    "    Input\n",
    "    seq_len:    int\n",
    "\n",
    "    Output\n",
    "    masking: np.array[[bool]]\n",
    "    \"\"\"\n",
    "    mask = np.triu(np.ones(seq_len, seq_len))\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944e450",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98792860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_block(context:   np.array,\n",
    "                         W_k:       np.array,\n",
    "                         W_q:       np.array,\n",
    "                         W_Vup:     np.array,\n",
    "                         W_Vdown:   np.array,\n",
    "                         training:  bool = True) -> tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Self-attention block\n",
    "\n",
    "    Input\n",
    "    context:   np.array,\n",
    "    W_k:       np.array,\n",
    "    W_q:       np.array,\n",
    "    W_Vup:     np.array,\n",
    "    W_Vdown:   np.array,\n",
    "    training:  bool = True\n",
    "\n",
    "    Output\n",
    "    tuple[output, attention_weight]\n",
    "    \"\"\"\n",
    "    context_lenght, embedding_dim = context.shape\n",
    "\n",
    "    K = context @ W_k # K: (32, 8)\n",
    "    Q = context @ W_q # Q: (32, 8)\n",
    "\n",
    "    attention_scores = (Q @ K.T) / np.sqrt(Q.shape[0]) # (32, 32)\n",
    "\n",
    "    # Masking attention scores and replace the lower triangle with -inf\n",
    "    causal_mask = crate_casual_mask(context_lenght)\n",
    "    attention_scores = np.where(causal_mask, attention_scores, -1e9)\n",
    "\n",
    "    attention_weights = softmax(attention_scores)\n",
    "\n",
    "    V_up = context @ W_Vup # (32, 8)\n",
    "\n",
    "    attention_output = attention_weights @ V_up # (32, 8)\n",
    "\n",
    "    output = attention_output @ W_Vdown # (32, 16)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1bb3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_block(context:  np.array,\n",
    "              W_up:     np.array,\n",
    "              b_up:     np.array,\n",
    "              W_down:   np.array,\n",
    "              b_down:   np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    MLP block\n",
    "\n",
    "    Input\n",
    "    context: np.array\n",
    "    W_up:    np.array\n",
    "    b_up:    np.array\n",
    "    W_down:  np.array\n",
    "    b_down:  np.array\n",
    "\n",
    "    Output\n",
    "    output: np.array\n",
    "    \"\"\"\n",
    "    hidden = context @ W_up + b_up\n",
    "    hidden = relu(hidden)\n",
    "    output = hidden @ W_down + b_down\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99007b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
